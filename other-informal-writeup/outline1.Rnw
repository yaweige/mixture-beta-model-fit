\documentclass[12pt]{article}         % the type of document and font size (default 10pt)
\usepackage[margin=1.0in]{geometry}   % sets all margins to 1in, can be changed
\usepackage{moreverb}                 % for verbatimtabinput -- LaTeX environment
\usepackage{url}                      % for \url{} command
\usepackage{amssymb}                  % for many mathematical symbols
\usepackage[pdftex]{lscape}           % for landscaped tables
\usepackage{longtable}                % for tables that break over multiple pages
\title{Outline of writeup}  % to specify title
\author{Yawei Ge}          % to specify author(s)


\begin{document}
\SweaveOpts{concordance=TRUE}
\setkeys{Gin}{width=1.0\textwidth}

\maketitle             

\section*{Outline 1}

\textbf {Questions answered: (Is this a good question defined?)}
\bigskip

The firearm examiners are focusing on the same source and different source problems of bullets and cartridge cases which serve as important forensic evidence in the court/to the jurors. The subjectivity in the traditional forensic process is called to be reduced or complemented by more objective methods. Some automatic matching algorithms are developed which usually return a similarity score (0-1) to quantify the similarity or the probability to be an actual match for a certain comparison. However, this raises questions about how to interpret the reported probabilities and how these probabilities are distributed. Thus, it is not all clear how to conduct inferences based on the similarity scores. [The paper] proposed binomial and beta-binomial for the number of matched cells of the CMC method. Thus, it provides a way to quantify the theoretical error rate of the algorithm. However, for the bullets LEA comparison, we haven’t established similar distributional results. In this paper, we will evaluate the possible models/distributions for the LEA comparisons scores generated by the random forest proposed by Hare etc. And then, we will also evaluate the error rates based on the estimated distribution for the automatic matching algorithm. (threshold selection could also be mentioned if needed).

Simply speaking, the question is “how is the LAPD RF score distributed and how to evaluate the theoretical error rate for this particular combination of ammunition and firearm based on your (our) proposed distributional form, further, how will the conclusion be affected by the sample sizes”

\textbf {Answer the question}
\bigskip

To answer the question, basically, we will choose the distributional form, estimate the distribution, evaluate the fit, calculate the error rate; change sample sizes and evaluate the sensitivity to the sample sizes

\begin{enumerate}
\item We first propose the candidate distributional forms
\item then choose the one best describing the data while gain a proper explanation
\item evaluate the fit
\item analyze the result
\item evaluate the error rate
\item sample size effect
\item conclude
\end{enumerate}

\textbf {The main results}

\bigskip

The Main results: which have been discussed for a while, need some modification, reproduce some results

\bigskip

\textbf {Sections of the writeup}

\begin{itemize}
  \item introduction, literature review
  \item distributions underlying the scores in Forensic applications
  \item data set (LAPD), the distribution estimated, test of sufficiency of single beta, and choose components
  \item the error rate analysis associated
  \item with changing sample sizes, evaluate the performance (in terms of stability, variation, and error rates)
  \item conclusions
\end{itemize}

\textbf {Work to do/Figures wanted/Table wanted/Result wanted}


1.	Generate RF scores for all data, make histograms.

2.	Report full data 2-component distributions, and 95\% bootstrap confidence interval.

3.	Draw the distributions, probably the CIs, probably rugs and other elements.

4.	Conduct test of sufficiency for single beta w.r.t more complex betamix, could use likelihood ratio? Could use CIs for single parameters? Make tables.

5.	Calculate some f-divergence statistics, but can we use them as formal tests? Maybe not.

6.	Decide the final distributional forms for the full data.

Error rate part:

7.	Since there are many ways of decide a threshold given similarities scores. I will make use of the one where KM and KNM have equal probability density, which is the way used in NIST CMC paper? But just to keep this simple to help reach our main goals.

8.	Define and evaluate the error rate

9.	Evaluate the individual error rate? Like the one done in the NIST CMC paper? Don’t seem to be needed in our case.

10.	Should we touch the error rate in bullet level?

Changing sample size:

11.	Compare estimation with mean, variance, f-divergence (maybe f-divergence helpful here)

12.	Also, draw plot like those we did last fall

13.	Report error rates in these cases

14.	May choose different models (beta) than the full if betamix used previously not suitable here

15.	Conclude the sample size effect


\end{document}

