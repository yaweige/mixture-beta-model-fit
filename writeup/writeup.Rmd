---
title: Estimating error rates for bullet comparisons in forensic science
author:
  - name: Yawei Ge
    email: yaweige@iastate.edu
    affiliation: ISU,CSAFE
    footnote: Corresponding Author
  - name: Heike Hofmann
    email: hofmann@iastate.edu
    affiliation: ISU,CSAFE
address:
  - code: ISU
    address: Department of Statistics, Iowa State University, 
  - code: CSAFE
    address: Center for Statistics and Applications of Forensic Evidence, Iowa State University
abstract: |
  This is the abstract.

  It consists of two paragraphs.

journal: "An awesome journal"
date: "`r Sys.Date()`"
bibliography: references.bib
#linenumbers: true
numbersections: true
csl: elsevier-harvard.csl
output: rticles::elsevier_article
header-includes:
  -  \usepackage{hyperref}
  -  \usepackage{graphicx}
  -  \usepackage[dvipsnames]{xcolor}
  -  \usepackage{multirow}
  -  \usepackage{diagbox}
  -  \usepackage{amsthm}
---


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\yg}[1]{{\textcolor{blue}{#1}}}



_Text based on elsarticle sample manuscript, see [http://www.elsevier.com/author-schemas/latex-instructions#elsarticle](http://www.elsevier.com/author-schemas/latex-instructions#elsarticle)_

```{r setup, warning=FALSE,message=FALSE, echo=FALSE}
library(dplyr)
library(ggplot2)
library(readr)
library(stringr)
library(kableExtra)
theme_set(theme_bw())
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
```

```{r data-input-section, include=FALSE}
# Here is all the input data, some are bullet data, some are used sequences or index as records.
# I call part1 for the model estimation part and
# part2 for the "application" section
ccf_data <- read.csv("../data/organized data/ccf-related/ccf_data.csv")
ccf_data_part1 <- ccf_data %>% filter(usage == "part1")
# Some land names is not well coded for part2, will be updated later 
ccf_data_part2 <- ccf_data %>% filter(usage == "part2")

# Used for the changing sample size section. These are randomly generated sequences, to reduce the randomness of each run, it is stored
change_size_sequences <- read.csv("../data/organized data/ccf-related/change_size_sequences.csv")

# The randomly generated test set
test_fau_bullet_index <- read.csv("../data/organized data/ccf-related/test_fau_bullet_index.csv")
```

```{r model-estimation-section, include=FALSE}
ccf_km_full_data <- ccf_data_part1 %>% filter(source == "KM")
ccf_knm_full_data <- ccf_data_part1 %>% filter(source == "KNM")

# only 35 knms out of 146880 knms are greater than (or equal to) 0.9, remove those
ccf_knm_full_data <- ccf_knm_full_data %>% filter(ccf < 0.90)

# The function to do the model fit====================
#par: starting values provided, different by each k
#par = c(a, b) when k = 1,
#par = c(p1, a1, b1, a2, b2), when k = 2
#par = c(p1, p2, a1, b1, a2, b2, a3, b3), when k =3
#k: number of components 1, 2 or 3.
#data: numeric vector
#control: provided to "optim"
#if the function produce warnings of NaN, that's not a problem, since I don't set hard bounds for parameters
#the "optim" function will handle that well
mine_betamix <- function(data, par, k, control = list(), hessian = FALSE){
  beta_likelihood <- function(par, data) {
    a1 <- par[1]
    b1 <- par[2]
    
    beta_1 <- dbeta(data, shape1 = a1, shape2 = b1)
    
    output <- -sum(log(beta_1))
  }
  
  betamix_likelihood <- function(par = c(0.5, 1, 1, 1, 1), data) {
    p1 <- par[1]
    a1 <- par[2]
    b1 <- par[3]
    a2 <- par[4]
    b2 <- par[5]
    
    if (p1<=0 | p1>=1 | a1<= 0 | b1 <= 0| a2<=0| b2<=0){
      output <- Inf
    } else if ((a1/(a1 + b1))>=(a2/(a2 + b2))) {
      output <- Inf
    } else {
      beta_1 <- dbeta(data, shape1 = a1, shape2 = b1)
      beta_2 <- dbeta(data, shape1 = a2, shape2 = b2)
      output <- -sum(log(p1*beta_1 + (1-p1)*beta_2))
    }
    
    output
  }
  
  beta_c3_likelihood <- function(par, data) {
    p1 <- par[1]
    p2 <- par[2]
    a1 <- par[3]
    b1 <- par[4]
    a2 <- par[5]
    b2 <- par[6]
    a3 <- par[7]
    b3 <- par[8]
    
    beta_1 <- dbeta(data, shape1 = a1, shape2 = b1)
    beta_2 <- dbeta(data, shape1 = a2, shape2 = b2)
    beta_3 <- dbeta(data, shape1 = a3, shape2 = b3)
    
    output <- -sum(log(p1*beta_1 + p2*beta_2 + (1 - p1 - p2)*beta_3))
  }
  
  if (k == 1) {
    output <- optim(par = par, fn = beta_likelihood, 
                    data = data,
                    control = control,
                    hessian = hessian)
  } else if (k == 2) {
    output <- optim(par = par, fn = betamix_likelihood, 
                    data = data,
                    control = control,
                    hessian = hessian)
  } else if (k == 3) {
    output <- optim(par = par, fn = beta_c3_likelihood, 
                    data = data,
                    control = control,
                    hessian = hessian)
  }
  
  output$value <- -output$value
  names(output)[which(names(output) == "value")] <- "logLik"
  output
}

# single beta============================
ccf_km_c1 <- mine_betamix(data = ccf_km_full_data$ccf, 
                          par = c(4.16, 2.38),
                          k = 1)
# be sure to check the convergence
ccf_km_c1$convergence

ccf_knm_c1 <- mine_betamix(data = ccf_knm_full_data$ccf, 
                           par = c(6.457, 9.54),
                           k = 1)
ccf_knm_c1$convergence

# two-component beta mixture=========================
ccf_km_c2 <- mine_betamix(data = ccf_km_full_data$ccf, 
                          par = c(0.3, 5, 5, 5, 2),
                          k = 2,
                          control = list(maxit = 1000))

ccf_km_c2$convergence

# starting values borrowed from betamix fit result, however, it doesn't need to be so informative,
# par = c(0.5, 5, 10, 5, 5) works well too
ccf_knm_c2 <- mine_betamix(data = ccf_knm_full_data$ccf, 
                           par = c(0.634, 11.5, 21, 6.8, 7.2),
                           k = 2,
                           control = list(maxit = 1000))
ccf_knm_c2$convergence

# three-component beta mixture========================

ccf_km_c3 <- mine_betamix(data = ccf_km_full_data$ccf,
                          par = c(0.548, 0.2, 7.5, 4.19, 10.69, 17, 26, 6),
                          k = 3,
                          control = list(maxit = 3000))
ccf_km_c3$convergence

ccf_knm_c3 <- mine_betamix(data = ccf_knm_full_data$ccf,
                           par = c(0.65, 0.314, 12, 23, 12.5, 12.5, 12.3, 6.2),
                           k = 3,
                           control = list(maxit = 3000))
ccf_knm_c3$convergence
# summary model params================================
#par = c(a, b) when k = 1,
#par = c(p1, a1, b1, a2, b2), when k = 2
#par = c(p1, p2, a1, b1, a2, b2, a3, b3), when k =3

param_extract <- function(model) {
  if(model$convergence != 0) warning("model not converged")
  par <- model$par
  nparam <- length(par)
  
  if(nparam == 2) {
    output <- data.frame(a = par[1],
                         b = par[2])
    output <- output %>% mutate(mu = a/(a+b),
                                phi = a+b)
    
  } else if (nparam == 5) {
    output <- data.frame(p1 = par[1],
                         a1 = par[2],
                         b1 = par[3],
                         a2 = par[4],
                         b2 = par[5])
    output <- output %>% mutate(mu1 = a1/(a1+b1),
                                phi1 = a1+b1,
                                mu2 = a2/(a2+b2),
                                phi2 = a2+b2)
  } else if (nparam == 8) {
    output <- data.frame(p1 = par[1],
                         p2 = par[2],
                         a1 = par[3],
                         b1 = par[4],
                         a2 = par[5],
                         b2 = par[6],
                         a3 = par[7],
                         b3 = par[8])
    output <- output %>% mutate(mu1 = a1/(a1+b1),
                                phi1 = a1+b1,
                                mu2 = a2/(a2+b2),
                                phi2 = a2+b2,
                                mu3 = a3/(a3+b3),
                                phi3 = a3+b3)
  } else {
    stop("The function can't handle this case")
  }
  return(output)
}

param_km_c1 <- param_extract(ccf_km_c1)
param_knm_c1 <- param_extract(ccf_knm_c1)
param_km_c2 <- param_extract(ccf_km_c2)
param_knm_c2 <- param_extract(ccf_knm_c2)
param_km_c3 <- param_extract(ccf_km_c3)
param_knm_c3 <- param_extract(ccf_knm_c3)

# simulation to plot===================================

# two-component betamix
myfit1_simulation <- data.frame(ccf = (1:99)/100, 
                                y1 = dbeta((1:99)/100, 
                                           shape1 = param_km_c2$a1, 
                                           shape2 = param_km_c2$b1),
                                y2 = dbeta((1:99)/100, 
                                           shape1 = param_km_c2$a2, 
                                           shape2 = param_km_c2$b2))

myfit1_simulation <- myfit1_simulation %>% mutate(y = param_km_c2$p1*y1 + (1-param_km_c2$p1)*y2)

myfit2_simulation <- data.frame(ccf = (1:99)/100, 
                                y1 = dbeta((1:99)/100, 
                                           shape1 = param_knm_c2$a1, 
                                           shape2 = param_knm_c2$b1),
                                y2 = dbeta((1:99)/100, 
                                           shape1 = param_knm_c2$a2, 
                                           shape2 = param_knm_c2$b2))

myfit2_simulation <- myfit2_simulation %>% mutate(y = param_knm_c2$p1*y1 + (1-param_knm_c2$p1)*y2)

myfit1n2_simulation <- bind_rows(myfit1_simulation, myfit2_simulation, .id = "Class")

# single beta
# one component
myfit3_simulation <- data.frame(ccf = (1:99)/100, 
                                y = dbeta((1:99)/100, 
                                          shape1 = param_km_c1$a, 
                                          shape2 = param_km_c1$b))

myfit4_simulation <- data.frame(ccf = (1:99)/100, 
                                y = dbeta((1:99)/100, 
                                          shape1 = param_knm_c1$a, 
                                          shape2 = param_knm_c1$b))

myfit3n4_simulation <- bind_rows(myfit3_simulation, myfit4_simulation, .id = "Class")

# statistics calculated for the models=========================

# likelihood ratio
lr1 <- 2*(ccf_km_c2$logLik - ccf_km_c1$logLik)
lr1p <- 1-pchisq(2*(ccf_km_c2$logLik - ccf_km_c1$logLik), df = 3)

lr2 <- 2*(ccf_km_c3$logLik - ccf_km_c2$logLik)
lr2p <- 1-pchisq(2*(ccf_km_c3$logLik - ccf_km_c2$logLik), df = 3)

lr3 <- 2*(ccf_knm_c2$logLik - ccf_knm_c1$logLik)
lr3p <- 1-pchisq(2*(ccf_knm_c2$logLik - ccf_knm_c1$logLik), df = 3)

lr4 <- 2*(ccf_knm_c3$logLik - ccf_knm_c2$logLik)
lr4p <- 1-pchisq(2*(ccf_knm_c3$logLik - ccf_knm_c2$logLik), df = 3)

# bic
bic1 <- 2*log(nrow(ccf_km_full_data))- 2*ccf_km_c1$logLik
bic2 <- 5*log(nrow(ccf_km_full_data))- 2*ccf_km_c2$logLik
bic3 <- 8*log(nrow(ccf_km_full_data))- 2*ccf_km_c3$logLik

bic4 <- 2*log(nrow(ccf_knm_full_data))- 2*ccf_knm_c1$logLik
bic5 <- 5*log(nrow(ccf_knm_full_data))- 2*ccf_knm_c2$logLik
bic6 <- 8*log(nrow(ccf_knm_full_data))- 2*ccf_knm_c3$logLik

```

```{r error-rate-section, include=FALSE,eval=FALSE}
pbetamix2 <- function(x, model){
  p1 <- model$par[1]
  p2 <- 1 - model$par[1]
  a1 <- model$par[2]
  b1 <- model$par[3]
  a2 <- model$par[4]
  b2 <- model$par[5]
  
  pdf <- p1*pbeta(x, shape1 = a1, shape2 = b1) + 
    p2*pbeta(x, shape1 = a2, shape2 = b2)
  
  pdf
}

dbetamix2 <- function(x, model){
  p1 <- model$par[1]
  p2 <- 1 - model$par[1]
  a1 <- model$par[2]
  b1 <- model$par[3]
  a2 <- model$par[4]
  b2 <- model$par[5]
  
  pdf <- p1*dbeta(x, shape1 = a1, shape2 = b1) + 
    p2*dbeta(x, shape1 = a2, shape2 = b2)
  
  pdf
}

find_cross <- function(m1, m2, start = 0.4){
  a <- 1
  b <- 2
  x <- start
  while(abs(a - b)>=0.001) {
    x <- x + 0.0001
    a <- dbetamix2(x, m1)
    b <- dbetamix2(x, m2)
    
    if (x > 0.7){
      stop("the cutoff exceeds 0.7, try smaller steps")
    }
  }
  x
}

# 0.529
cutoff1 <- find_cross(ccf_km_c2, ccf_knm_c2, 0.4)

# numbers as a reference but changed a little for now
# FPR: 0.148
fdr <- 1 - pbetamix2(cutoff1, ccf_knm_c2)
# FNR: 0.301
fnr <- pbetamix2(cutoff1, ccf_km_c2)
# FIR: 0.212
fir <- (1 - pbetamix2(cutoff1, ccf_knm_c2))/(1 - pbetamix2(cutoff1, ccf_km_c2))
# FER: 0.354
fer <- pbetamix2(cutoff1, ccf_km_c2)/pbetamix2(cutoff1, ccf_knm_c2)


```

```{r application-section, include=FALSE,eval=FALSE}
# some abuse of notations in part2 data
# the "nonmatched" in the name refer to the case one bullet with a different source compared with three same source bullets. The "KM" and "KNM" as a variable indicating the way we treat those comparisons. "KNM" refers to "KNM" the same as before. "KM" here doesn't mean all of them is known-matched, for those comparisons among the three same source bullets, they are, for those involving the one different bullet, they are not. But the "KM" involving the one different bullet refers to those lands we suspect to be matched in those bullet comparisons.

# Use part1 data: database, same-source comparison (True results: all same source comparisons)=========
test_index <- test_fau_bullet_index$FAU
questioned_bullet_index <- test_fau_bullet_index$bullet

# introduce FAU and bullet in ccf_km_full_data, ccf_knm_full_data
ccf_km_full_data <- ccf_km_full_data %>% 
  mutate(FAU1 = as.numeric(str_extract(land1, pattern = "(?<=LAPD - (FAU )?)\\d+")),
         FAU2 = as.numeric(str_extract(land2, pattern = "(?<=LAPD - (FAU )?)\\d+")))

ccf_knm_full_data <- ccf_knm_full_data %>% 
  mutate(FAU1 = as.numeric(str_extract(land1, pattern = "(?<=LAPD - (FAU )?)\\d+")),
         FAU2 = as.numeric(str_extract(land2, pattern = "(?<=LAPD - (FAU )?)\\d+")))

# split out the test set (from the database set), FAU1 = FAU2 here. The list is listed in numeric order.
test_km_sets <- ccf_km_full_data %>%
  filter(as.character(FAU1) %in% as.character(test_index)) %>%
  split(f = .$FAU1)

test_knm_sets <- ccf_knm_full_data %>%
  filter(as.character(FAU1) %in% as.character(test_index)) %>%
  split(f = .$FAU1)

if (any(names(test_km_sets) != names(test_knm_sets))) stop("FAUs not matched in application section")

# split the questioned bullet out in each set
# for now, the 2 barrels not in the data. This will be solved after updating the data later
test_questioned_sets <- Map(function(x, y, z) {
  
  test_km <- x %>% filter(bullet1 != z, bullet2 != z)
  test_knm <- y %>% filter(bullet1 != z, bullet2 != z)
  questioned_km <- x %>% filter(bullet1 == z|bullet2 == z)
  
  output <- list(test_km = test_km, test_knm = test_knm,
                 questioned_km = questioned_km)
  
}, test_km_sets, test_knm_sets, questioned_bullet_index[1:98])

# fit the distributions for each set (three test fires)
case_by_case_models <- lapply(test_questioned_sets, FUN = function(x) {
  # the starting values are those from the full data models
  km_model <- mine_betamix(data = x$test_km$ccf, 
                           par = c(0.419, 7.37, 8.45, 14.03, 4.45),
                           k = 2,
                           control = list(maxit = 3000),
                           hessian = FALSE)
  knm_model <- mine_betamix(data = x$test_knm$ccf, 
                            par = c(0.666, 11.23, 20.33, 7.06, 7.22),
                            k = 2,
                            control = list(maxit = 3000),
                            hessian = FALSE)
  
  threshold_equal_pdf <- find_cross(km_model, knm_model,start = 0.2,end = 0.9, step = 0.0001)
  
  output <- list(km_model = km_model, knm_model = knm_model, threshold_equal_pdf = threshold_equal_pdf)
  output
})

# The following check of convergence is not reported
# KM convergence results, 1 indicates not converged
case_by_case_models %>% map_dbl(.f = function(x) x$km_model$convergence)
# KNM convergence results
case_by_case_models %>% map_dbl(.f = function(x) x$knm_model$convergence)
# The ones we can use: (both knm and km are converged)
converged_index <- case_by_case_models %>% map_dbl(.f = function(x) x$km_model$convergence)==0 &
  case_by_case_models %>% map_dbl(.f = function(x) x$knm_model$convergence) == 0

case_by_case_classifcation <- Map(function(x, y) {
  # True: identification; False: elimination
  class <- x$questioned_km$ccf > y$threshold_equal_pdf
  class
}, test_questioned_sets, case_by_case_models)

case_by_case_classifcation_converged <- subset(case_by_case_classifcation, converged_index)
case_by_case_classifcation_converged %>% unlist() %>% sum
case_by_case_classifcation_converged %>% unlist() %>% length

# Use part2 data(True results: all different source comparisons)=================================

ccf_nonmatched_km_data <- ccf_data_part2 %>% filter(source == "KM")
ccf_nonmatched_knm_data <- ccf_data_part2 %>% filter(source == "KNM")

# For the current coding method, it's not available to get all FAU1 and FAU2
# but the following would work for the purpose of split (the following step) (to get list forms)

# The fau 246 is used, however, it is labeled as 245 in the land names, so we took care of that by using the current way to split.
ccf_nonmatched_km_data <- ccf_nonmatched_km_data %>% 
  mutate(FAU1toSplit = str_extract(land1, pattern = "(?<=LAPD - (FAU )?)\\d+"),
         FAU2toSplit = str_extract(land2, pattern = "(?<=LAPD - (FAU )?)\\d+"),
         FAUtoSplit = ifelse(is.na(FAU1toSplit), yes = FAU2toSplit, no = FAU1toSplit))

ccf_nonmatched_knm_data <- ccf_nonmatched_knm_data %>% 
  mutate(FAU1toSplit = str_extract(land1, pattern = "(?<=LAPD - (FAU )?)\\d+"),
         FAU2toSplit = str_extract(land2, pattern = "(?<=LAPD - (FAU )?)\\d+"),
         FAUtoSplit = ifelse(is.na(FAU1toSplit), yes = FAU2toSplit, no = FAU1toSplit))


ccf_nonmatched_km <- ccf_nonmatched_km_data %>%
  split(f = rep(1:98, each = 36))

ccf_nonmatched_knm <- ccf_nonmatched_knm_data %>%
  split(f = rep(1:98, each = 240))

nonmatched_km_fau_names <- lapply(ccf_nonmatched_km, FUN = function(x){
  output <- unique(x$FAUtoSplit[!is.na(x$FAUtoSplit)])
  if (length(output) != 1) stop("Incorrect split in application section for part2 data")
  output
}) %>% unlist()

nonmatched_knm_fau_names <- lapply(ccf_nonmatched_knm, FUN = function(x){
  output <- unique(x$FAUtoSplit[!is.na(x$FAUtoSplit)])
  if (length(output) != 1) stop("Incorrect split in application section for part2 data")
  output
}) %>% unlist()

if (any(nonmatched_km_fau_names != nonmatched_knm_fau_names)) stop("Incorrect split in application section for part2 data")
names(ccf_nonmatched_km) <- nonmatched_km_fau_names
names(ccf_nonmatched_knm) <- nonmatched_knm_fau_names

# To fit the models
nonmatched_test_questioned_sets <- Map(function(x, y) {
  
  test_km <- x %>% filter(bullet1 != "Q", bullet2 != "Q")
  test_knm <- y %>% filter(bullet1 != "Q", bullet2 != "Q")
  questioned_km <- x %>% filter(bullet1 == "Q"|bullet2 == "Q")
  
  output <- list(test_km = test_km, test_knm = test_knm,
                 questioned_km = questioned_km)
  
}, ccf_nonmatched_km, ccf_nonmatched_knm)

nonmatched_test_questioned_sets[[1]]

nonmatched_case_by_case_models <- lapply(nonmatched_test_questioned_sets, FUN = function(x) {
  # the starting values are those from the full data models
  km_model <- mine_betamix(data = x$test_km$ccf, 
                           par = c(0.419, 7.37, 8.45, 14.03, 4.45),
                           k = 2,
                           control = list(maxit = 3000),
                           hessian = FALSE)
  knm_model <- mine_betamix(data = x$test_knm$ccf, 
                            par = c(0.666, 11.23, 20.33, 7.06, 7.22),
                            k = 2,
                            control = list(maxit = 3000),
                            hessian = FALSE)
  
  threshold_equal_pdf <- find_cross(km_model, knm_model,start = 0.2,end = 0.9, step = 0.0001)
  output <- list(km_model = km_model, knm_model = knm_model, threshold_equal_pdf = threshold_equal_pdf)
  output
})

# Similarly, check the convergence
# KM convergence results, 1 indicates not converged
nonmatched_case_by_case_models %>% map_dbl(.f = function(x) x$km_model$convergence)
# KNM convergence results
nonmatched_case_by_case_models %>% map_dbl(.f = function(x) x$knm_model$convergence)
# The ones we can used: (both knm and km are converged)
nonmatched_converged_index <- nonmatched_case_by_case_models %>% map_dbl(.f = function(x) x$km_model$convergence)==0 &
  nonmatched_case_by_case_models %>% map_dbl(.f = function(x) x$knm_model$convergence) == 0


nonmatched_case_by_case_classifcation <- Map(function(x, y) {
  # True: identification; False: elimination
  class <- x$questioned_km$ccf > y$threshold_equal_pdf
  class
}, nonmatched_test_questioned_sets, nonmatched_case_by_case_models)

# The above code is for case-specific scenario. Now for the database scenario==================
test_km_sets <- ccf_km_full_data %>%
  filter(as.character(FAU1) %in% as.character(test_index)) %>%
  split(f = .$FAU1)

database_matchedB_km <- mine_betamix(data = ccf_km_full_data %>%
                                       filter(!as.character(FAU1) %in% as.character(test_index)) %>%
                                       as.data.frame() %>%
                                       pull(ccf),
                                     par = c(0.419, 7.36, 8.45, 14, 4.45),
                                     k = 2,
                                     hessian = FALSE,
                                     control = list(maxit = 1000))
database_matchedB_km$converge 
database_matchedB_km

database_matchedB_knm <- mine_betamix(data = ccf_knm_full_data %>%
                                       filter(!as.character(FAU1) %in% as.character(test_index)) %>%
                                       as.data.frame() %>%
                                       pull(ccf),
                                      par = c(0.666, 11.23, 20.33, 7.065, 7.22),
                                      k = 2,
                                      hessian = FALSE,
                                      control = list(maxit = 1000))


database_matchedB_knm$converge 
database_matchedB_knm

database_threshold <- find_cross(m1 = database_matchedB_km, m2 = database_matchedB_knm)
###

database_matchedB_km_result <- lapply(test_questioned_sets, 
                                      FUN = function(x) x$questioned_km$ccf > database_threshold)

database_matchedB_km_result %>% subset(converged_index) %>% unlist() %>% sum() 


# non-matched bullets
database_nonmatchedB_result <- lapply(nonmatched_test_questioned_sets, FUN = function(x) x$questioned_km$ccf > database_threshold )



# turn the data unchanged
ccf_km_full_data <- ccf_km_full_data %>% 
  select(-FAU1, -FAU2)

ccf_knm_full_data <- ccf_knm_full_data %>%
  select(-FAU1, -FAU2)
```

```{r sample-size-section, include=FALSE,eval=FALSE}
# make use of different starting values (this actually has no significant effect, and only part of it is shown in the final report)
# the full model one:
starting_1 <- matrix(c(param_km_c2$p1,  param_km_c2$a1,  param_km_c2$b1, param_km_c2$a2, param_km_c2$b2,
                       param_knm_c2$p1,  param_knm_c2$a1,  param_knm_c2$b1, param_knm_c2$a2, param_knm_c2$b2),
                     nrow = 2, byrow = TRUE, 
                     dimnames = list(class = c("KM", "KNM"), param = c("p1", "a1", "b1", "a2", "b2")))

starting_1

# kind of non-informative (some 6's to let the mu1 < mu2)
starting_2 <- matrix(c(0.3, 5, 6, 5, 5,
                       0.7, 5, 6, 5, 5),
                     nrow = 2, byrow = TRUE, 
                     dimnames = list(class = c("KM", "KNM"), param = c("p1", "a1", "b1", "a2", "b2")))
starting_2

# The function to do the fit
# For example: example <- change_size_sequence_fit(ccf_km_full_data, seq = change_size_sequences$seq1, start = starting_1[1,], class = "km")
change_size_sequence_fit <- function(data, seq, start = c(0.3, 5, 6, 5, 5), k = 2, class) {
  if (class == "km") {
    step <- 36
  } else if (class == "knm") {
    step <- 240
  } else {
    stop("class not one of km or knm")
  }
  model_list <- lapply(seq_along(seq), FUN = function(i) {
    index_start <- sort(seq[1:i]-1)*step+1
    index_end <- sort(seq[1:i])*step
    index <-  Map(f = function(x, y) {
      output <- x:y
      output
    }, index_start, index_end) %>% unlist
    
    used_data <- data$ccf[index]
    model <- mine_betamix(data = used_data,
                          par = start,
                          k = k,
                          control = list(maxit = 3000),
                          hessian = FALSE)
    
  })
  model_list
}


# The run=================================================
km_st1_sq1to10_100 <- lapply(1:10, FUN = function(x) {
  output <- change_size_sequence_fit(ccf_km_full_data, 
                                     seq = change_size_sequences[,i][1:100], 
                                     start = starting_1[1,], 
                                     class = "km")
})

knm_st1_sq1to10_100 <- lapply(1:10, FUN = function(i) {
  output <- change_size_sequence_fit(ccf_knm_full_data, 
                                     seq = change_size_sequences[,i][1:100], 
                                     start = starting_1[2,], 
                                     class = "knm")
})

# Analysis of results=====================================================

# seq: which sequence used
# sets: number of sets used
long_form_result_trans <- function(result) {
  output1 <- lapply(result, FUN = function(sets) {
    output2 <- lapply(sets, FUN = function(x) {
      output3<- data.frame(p1 = x$par[1],
                           a1 = x$par[2],
                           b1 = x$par[3],
                           a2 = x$par[4],
                           b2 = x$par[5])
      output3
    }) %>% bind_rows(.id = "sets")
    
    output2 <- output2 %>%
      mutate(mu1 = a1/(a1 + b1),
             mu2 = a2/(a2 + b2),
             mu = p1*mu1 + (1-p1)*mu2,
             var1 = a1*b1/(a1+b1)^2/(a1+b1+1),
             var2 = a2*b2/(a2+b2)^2/(a2+b2+1),
             var = p1^2*var1 + (1-p1)^2*var2)
    output2
  })
  output1
}

change_size_sequence_knm1to10_data <- long_form_result_trans(knm_st1_sq1to10_100) %>% bind_rows(.id = "seq")
change_size_sequence_km1to10_data <- long_form_result_trans(km_st1_sq1to10_100) %>% bind_rows(.id = "seq")

# To get thresholds, we need to match the KM and KNM models in using same "seq" and same "sets" used

threshold_newcol <- lapply(1:1000, FUN = function(i){
  data <- bind_rows(change_size_sequence_km1to10_data,
                    change_size_sequence_knm1to10_data)
  kmrow <- data[i, ]
  knmrow <- data[i+1000, ]
  fakekmmodel <- list(par = c(kmrow[3:7]) %>% unlist)
  fakeknmmodel <- list(par = c(knmrow[3:7]) %>% unlist)
  
  threshold <- find_cross(m1 = fakekmmodel, m2 = fakeknmmodel,
                          start = 0.2, end =  0.9, step = 0.0001)
  threshold
}) %>% unlist

change_size_sequence_data <- bind_rows(change_size_sequence_km1to10_data,
                                       change_size_sequence_knm1to10_data)
change_size_sequence_data$threshold <- c(threshold_newcol, threshold_newcol)
change_size_sequence_data$class <- rep(c("KM", "KNM"), each = 1000)

# bd column: Bhattacharyya Distance
# compare the estimated model with the model estimated with all available data: ccf_km_c2, ccf_knm_c2
bd_part1 <- lapply(1:1000, FUN = function(i) {
  therow <- change_size_sequence_data[i, ]
  fakemodel <- list(par = c(therow[3:7]) %>% unlist)
  
  f_integrate <- function(x) {
    sqrt(dbetamix2(x, ccf_km_c2)*dbetamix2(x, fakemodel))
  }
  
  output <- integrate(f_integrate, lower = 0.001, upper = 0.999)[[1]]
  output <- -log(output)
}) %>% unlist

bd_part2 <- lapply(1001:2000, FUN = function(i) {
  therow <- change_size_sequence_data[i, ]
  fakemodel <- list(par = c(therow[3:7]) %>% unlist)
  
  f_integrate <- function(x) {
    sqrt(dbetamix2(x, ccf_knm_c2)*dbetamix2(x, fakemodel))
  }
  
  output <- integrate(f_integrate, lower = 0.001, upper = 0.999)[[1]]
  output <- -log(output)
}) %>% unlist

change_size_sequence_data$bd <- c(bd_part1, bd_part2)
```

```{r prepare-data}
# two-component betamix
myfit1_simulation <- data.frame(ccf = (1:99)/100, 
                                y1 = dbeta((1:99)/100, shape1 = 7.366, shape2 = 8.450),
                                y2 = dbeta((1:99)/100, shape1 = 14.026, shape2 = 4.451))

myfit1_simulation <- myfit1_simulation %>% mutate(y = 0.41938*y1 + (1-0.41938)*y2)

myfit2_simulation <- data.frame(ccf = (1:99)/100, 
                                y1 = dbeta((1:99)/100, shape1 = 11.0696, shape2 = 19.8388),
                                y2 = dbeta((1:99)/100, shape1 = 6.5794, shape2 = 6.745))

myfit2_simulation <- myfit2_simulation %>% mutate(y = 0.6737*y1 + (1-0.6737)*y2)

myfit1n2_simulation <- bind_rows(myfit1_simulation, myfit2_simulation, .id = "Class")

# single beta
# one component
myfit3_simulation <- data.frame(ccf = (1:99)/100, 
                                y = dbeta((1:99)/100, shape1 = 4.1447, shape2 = 2.384))

myfit4_simulation <- data.frame(ccf = (1:99)/100, 
                                y = dbeta((1:99)/100, shape1 = 6.38, shape2 = 9.42))

myfit3n4_simulation <- bind_rows(myfit3_simulation, myfit4_simulation, .id = "Class")
```

```{r prepare-data2}
ccf_km_full <- readRDS("../data/organized data/ccf-related/ccf_km_full.rds")
ccf_knm_full <- readRDS("../data/organized data/ccf-related/ccf_knm_full.rds")

ccf_km_full_data <- bind_rows(ccf_km_full)
ccf_knm_full_data <- bind_rows(ccf_knm_full)

# Currently, we removed these lands (some of them are known to be mis-scanned)
# only 29 knms out of 106080 knms are greater than 0.9, remove those
ccf_knm_full_data <- ccf_knm_full_data %>% filter(ccf < 0.90)
```

\hh{Next steps:}

1.  for any image that you include with include_graphics("image.png") write a ggsave("image.png") in the code
2. Let's use darkgrey for different source and darkorange for same source comparisons
3. Switch from known match and known non-match to (known) same source and (known) different source
4. Whenever you show results - number estimates or figures, make sure to include in the description what bullets/barrels these results are based on. 
5. in the section on the LAPD data define what you mean by 'full data' and what sampling schemes you use later on.

\yg{1. Introduction}

\yg{2. Background}

\yg{3. Methods and data (no result presented)}

\yg{3.1 Distributional models}

\yg{3.2 Error rate analysis}

\yg{3.3 Data}

\yg{3.4 Database model v.s. case specific model}

\yg{3.4.1 Study design}

\yg{3.4.2 Training set and testing set}

\yg{3.5 Sample size effect and convergence}

\yg{4. Results and discussion}

\yg{4.1 Model result}

\yg{4.2 Error rate result}

\yg{4.3 Database model v.s. case specific model result}

\yg{4.4 Sample size effect and convergence result}


Test to show some result

`r param_km_c1$a` `r param_km_c1`

Reproduce Table 1 with `kableExtra`

```{r}
table1_data <- data.frame(Model = c("Beta", "2-comp", "2-comp", rep("3-comp", 3)),
                          `Component Prior Probability` = c(" ", 
                                                            as.character(round(param_km_c2$p1, digits = 2)),
                                                            as.character(round(1-param_km_c2$p1, digits =2)),
                                                            as.character(round(param_km_c3$p1, digits = 2)),
                                                            as.character(round(param_km_c3$p2, digits =2)),
                                                            as.character(round(1-param_km_c3$p1-param_km_c3$p2, digits = 2))),
                          mu = c(round(param_km_c1$mu, digits = 2), 
                                 round(param_km_c2$mu1, digits = 2),
                                 round(param_km_c2$mu2, digits = 2),
                                 round(param_km_c3$mu1, digits = 2), 
                                 round(param_km_c3$mu2, digits = 2),
                                 round(param_km_c3$mu3, digits = 2)),
                          phi = c(round(param_km_c1$phi, digits = 2), 
                                 round(param_km_c2$phi1, digits = 2),
                                 round(param_km_c2$phi2, digits = 2),
                                 round(param_km_c3$phi1, digits = 2), 
                                 round(param_km_c3$phi2, digits = 2),
                                 round(param_km_c3$phi3, digits = 2)))
names(table1_data) <- c("Model", "Component Prior Probability", "$\\mu$", "$\\phi$")

kbl(table1_data, booktabs = T, align = "c", escape = F) %>%
  column_spec(1, bold = T) %>%
  collapse_rows(columns = 1, latex_hline = "major", valign = "middle")
```

Introduction
====================================

Firearm examination is an important topic in forensic science to answer the questions, if two pieces of firearm evidence are from a same source. This examination is based on class and individual characteristics left by the firearm on bullets and cartridge cases.  Under current state of the art, these examinations are executed in forensic labs by Firearms and Toolmark Examiners (FTEs) under the regulation of AFTE [@AFTE1992], and involves visual inspection of similarities, generally with the help of a comparison microscope. 
This process is in its nature based on human decisions and therefore subjective. The National Research Council criticized this subjectivity in the conventional firearm identification process and called for it to be reduced or to be complemented by more objective procedures [@Council2009]. The Presidentâ€™s Council of Advisers on Science and Technology (PCAST) also emphasized the importance to establish the scientific validity and reliability of subjective forensic feature-comparison methods by blind empirical testing [@PCAST2016]. Additionally, the PCAST report recognized the importance of developing objective computer-based algorithms in its following addendum [@PCAST2016addendum].

In response to this criticism,  research on cartridge case comparisons introduced the method of  congruent matching cells (CMC) [@Song2015], which allows a quantitative assessment of the similarity of two breech face impressions. Theoretical error rates of CMC were analyzed by @Song2018 and @Zhang2019 on the data set of @Fadul2011 based on variants of a binomial model. 
In bullet comparisons, a fully automatic matching algorithm was proposed by @Hare2016, its error rates  <!--The discussion about the degraded land comparison was also made in the following paper [@Hare2017].--> are empirically evaluated and  discussed by @Vanderplas2020  based on three test sets. 
The most prominent single feature in the random forest model @Hare2016  proposed is the cross-correlation function (CCF), previously identified as important in other methods of similarity assessment [@chumbley; @vorburger2011; @Krishnan2019; @cmps]. 
<!--Other automated comparison methods or improvement such as Robust LOESS in groove engraved area identification [@Rice2020], Chumbley score method [@Krishnan2019] are also proposed. -->
One less well investigated aspect of the cross-correlation measure, however, is its distribution.

Due to  limitations of sample sizes and study designs, sound distributional assumptions about the resulting similarity scores are not well established in the bullet comparison field for automated matching algorithms. Therefore, error rates are generally estimated empirically. This estimation is often based on only dozens of bullets from a selected few firearms. <!--originally used in case studies to estimate the current lab practice's error rates. -->
<!-- \hh{I'm not sure how the next factoid is relevant} And many of those tests are also criticized for the closed-set design and potential information provided to the examiners as pointed out by PCAST [@PCAST2016; @PCAST2016addendum].-->
To estimate the theoretical error rates of any automatic algorithm, it is important to understand the underlying distributions of the similarity scores. \hh{XXX add a few citations for papers discussing score-based likelihood ratios.} These distributions also play a key role in an analysis of  score-based likelihood ratios (SLR), used to quantify the strength of  similarities and unify the measurement of strength of evidence. 
In this paper, we build beta mixture distribution models for the cross-correlation function (CCF) to provide a framework for error rate estimations and score-based likelihood ratio analysis of the output of automatic algorithms. We also evaluate the size of data needed to reach good estimations for the distributions. We show the size of the data is crucial in applying score-based likelihood ratios and thus, relevant reference databases are needed and required to exceed the minimal number of specimens.


<!-- \hh{Don't start sentences with `And`. I am also not sure, what you refer to by `that`. }
And that also helps later \hh{in this paper?} to choose proper thresholds for the methods to reduce the overall error rates instead of simply choosing a threshold to distinguish between same and different sources. \hh{first define thresholds and the need to get from scores to actionable conclusions.}
-->


\hh{XXX Let's end the introduction here with a  paragraph on the structure of the rest of the paper.}

In section 2... In section 3...


Background
============================


\hh{A sketch of a bullet and a barrel would be good here - showing rifling (grooves and lands) and striation marks. Introduce rifling, Describe LEA and GEA, and structure of LEAs}

Generally, in forensic science, we consider the problem of determining for two pieces of evidence whether they originate from the same source or from different sources. 
Specifically, when bullet evidence is compared, it is of interest if two bullets were fired from the same gun. 
For firearms, there is rifling machined into the internal of the barrels aiming to improve the aerodynamic stability of bullets. During the firing process, microimperfections in a barrel leave markings on a bullet. These  markings are thought to be unique and are used as the basis to determine the source of the bullets. 
<!--For forensic purpose, we distinguish between two different levels of characteristics. Characteristics shared by a type of guns and bullets are called class characteristics, such as the number of lands, rifling directions or type of rifling. Different class characteristics lead to a conclusion of `elimination`. Class characteristics can never be used to identify a same source. The characteristics corresponding to a particular firearm are called individual characteristics and are used to identify the source of the bullets.--> 
For bullet evidence, in particular, striations on land engraved areas (LEAs) are used by firearms and toolmarks examiners to determine same source of two pieces of evidence [@AFTE1992]. <!--The grooves engraved area (GEAs) are the markings left by the grooves between two lands. We usually don't use GEAs directly for the identification purpose but we usually need to distinguish between GEAs and LEAs especially for the computer based algorithms.-->
For computer based algorithms, 3D scans of the LEAs are used. <!-- to carry more information than traditional 2D scans.-->
Following the process described in @Hare2016, we extract **signatures** \hh{XXX pictures!} from these scans. Various  scores are used in the literature to describe the similarity between two signatures [@ma2004; @nichols1997; @nichols2003; @Hare2016].
Here, we will employ the cross-correlation function (CCF) to measure similarity.
The cross-correlation function (CCF) is calculated by maximizing the  correlation of two  signatures under a vertical shift.

Mathematically, CCF is  a value between -1 and 1, with larger values indicating higher similarity between pairs of signatures. Discriminating between same-source and different-source pairs of signatures is based on a cutoff value. Naively, one could assign positive CCF values to indicate same-source pairs and, similarly, negative values to different-source pairs. However, in practice it turns out that values of CCF based on pairs of bullet land signatures are not symmetric around the origin, but mostly positive regardless of source. Negative values are rarely observed for LEA comparisons in practice (and in none of the more than 100k comparisons considered in this paper).
Moreover, different pairings of firearm and ammunition result in different distributions of values, therefore no single CCF value is sufficient to distinguish between same-source and different-source pairs. Alternatively to using scores directly, \hh{XXXX citations XXX} suggest the use of score-based log-likelihood ratios to evaluate the strength of forensic evidence. 

<!--This provides us with a rich tool box and help the field advance, but it comes with issues. For the practitioners not in statistical major who don't fully understand the differences among those algorithms, a same number represents different strength of evidence is extremely confusing. And even in the academic field, we need a unified way to quantify the strength of evidence considering different behaviors of different scores. The score based on likelihood ratios is proposed in forensic science to measure the strength of evidence.-->

Score-based log-likelihood ratios (SLR) quantify how much more likely one outcome is than the other one. SLRs are used for their well-understood statistical properties and large-sample properties, which allow to both evaluate prediction error rates and control them.
<!--\yg{
And it represents how much one result is more likely than the other one and comes with great statistical properties in terms of minimizing losses and achieving large sample properties.}--> 
<!--The key of making use of likelihood ratios is to understand the underlying theoretical distributions of each particular type of scores. Intuitively, without knowing how a type of score is distributed, it makes no sense to make any claim for a particular value. For example, the random forest method in the bullet comparison gives very separated scores, either close to one or close to zero, and it actually achieves good classification results. But it doesn't mean it won't make an error, it makes error even when the score close to zero which we might expect to be different source pairs. But when CCF achives that score close to 0, we hardly make wrong claims. It's because the difference of distributions of each score and the absolute value itself could be misleading. And the prediction error rates should be evaluated and controlled at the same time.-->

<!-- And the LEA is also used in the developed computer-based methods. So, we are not doing one step to identify bullets, instead we consider each LEA and then combine the land level conclusions to a bullet level one. Accordingly, we start our discussion about underlying distributions and error rates from the land level. -->

Methods and data
============================

The distributional forms of similarity scores
----------------------------

Two  distributions are of immediate interest in the case of bullet comparisons: (1) the known-match (KM) or same-source distribution for similarity scores of same-source pairs of LEA scans, and (2)  the known-non-match (KNM) or different-source distribution for similarity scores for pairs of LEA scans from different sources. Any decision on the source of a comparison pair is then based on making a choice between those two  distributions. 
The strength of any identification is also measured by the disparities of those distributions. 
However, those two distributions are only available in experimental settings where we know the ground truth.

<!-- The quantitative methods used to objectively measure the similarity between LEAs report various quantities, such as counts, correlations, distances, probabilities and more general similarity scores [@ma2004; @nichols1997; @nichols2003; @Hare2016]. To understand how those quantities reflect the strength of evidence and to study the underlying error rates in making decisions based on those quantities, distributional forms are usually set up [@Zhang2019; @Song2018]. Particularly, we are focusing on the similarity scores which range from 0 to 1. The similarity scores reported in the forensic researches are classified into two categories as known matches (KM) and known non-matches (KNM), and the corresponding distributions are named as KM distributions and KNM distributions. When we make any decisions based on any quantitative measurement, we are actually making a choice between those two potential distributions. The strength of any identification process is also measured by the disparities of those distributions. However, in practice, we can hardly discriminate those two distributions entirely, thus, we are never 100% sure which distribution the observed score comes from. This is where the identification error raises.  -->

<!-- The cross-correlation function (CCF) is the most prominent single feature in the automatic random forest algorithm [@Hare2016] which has theoretical range from -1 to 1. But in real applications, this similarity score always has values in 0 to 1, e.g. in our case, none of 121992 comparisons has negative values. So, it is selected as a representative of similarity scores with range from 0 to 1 in this paper for further analysis. The similarity scores in 0 to 1 can be explained as probabilities that quantify the likelihood that a pair of LEAs are actually a match. Or we can think of them as general similarity measurement. As the name indicated, the higher the similarity score is, the stronger evidence is to support the same source assumption. For different combination of ammunition and firearms, the scores are distributed differently. It is expected that systematic differences exist there for different cases [@Vanderplas2020]. So, it is necessary to study the scores under controlled conditions. -->

```{r example, out.width="90%", fig.align="center", fig.cap="Histograms of the cross-correlation functions (CCFs) for the known-matched and known-nonmatched comparisons respectively for the LAPD data set. The histograms imply that the KNM distribution has a mode below 0.5 while the KM distribution peaks in a value above 0.5. The KM distribution also shows a heavy tail to the left or possibly a potential second mode", fig.keep="hold", fig.pos="h", fig.height = 4, fig.width=6}
myfit3n4_simulation %>%
  ggplot(aes(color = Class, fill = Class)) +
  geom_histogram(aes(x = ccf, y = ..density..), binwidth = 0.05, #color = "gray99",
                 position = "identity", alpha = 0.5, 
                 data = bind_rows(ccf_km_full_data, ccf_knm_full_data, .id = "Class")) +
  # geom_density(aes(x = ccf, y = ..density..), #color = "gray99",
  #                position = "identity", alpha = 0.5,
  #                data = bind_rows(ccf_km_full_data, ccf_knm_full_data, .id = "Class")) +
  scale_color_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  scale_fill_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  ylab("Density") +
  xlab("CCF")
```

<!-- \autoref{fig:example} \hh{shows the densities of cross-correlation values for both known-matching pairs and known non-matching pairs of bullet land-engraved areas (LEAs). XXX what data are you using? XXX The two densities exhibit a fairly typical pattern,} in the sense, that \hh{the distribution for known non-matching (KNM) pairs has a mode for CCF values below 0.5 and the distribution of CCF values for KM pairs peaks in a value above 0.5. }  -->

\autoref{fig:example} shows the histograms of cross-correlation values for both known-matching pairs and known non-matching pairs of bullet land-engraved areas (LEAs). The two histograms exhibit a fairly typical pattern, in the sense, that the distribution for known non-matching (KNM) pairs has a mode for CCF values below 0.5 and the distribution of CCF values for KM pairs peaks in a value above 0.5.

<!-- which is a typical \yg{pattern} we have for the CCFs of LEA comparisons, that the distributions of KM and KNM \yg{both have relatively high probabilities  for values below and above 0.5. Therefore, there is not a clear separation of the histograms of the same-source and different-source comparisons of the LEA scans.} -->

<!--In the bullet LEA comparison problems, we usually have well separated bullet scores \yg{but not for the land scores according to} @Vanderplas2020.-->

\yg{To capture the features of the distributions which are partly revealed in \autoref{fig:example}, we consider the beta distribution family to account for the unimodal pattern shown in the KNM distribution.}
<!-- \hh{XXX be careful with how you use the plural here. The Beta distribution is one family of distributions. You are assuming that both KM and KNM scores follow a beta distribution. } -->
<!-- \hh{XXX often-used is not enough - the distribution also needs to fit. It helps that the Beta distribution is quite flexible in what can be described. Move the flexibility up.}  -->
\yg{The beta distributions are very flexible in capturing unimodal asymmetric densities on the $[0,1]$ interval.}
<!-- The beta distribution is  often-used in Bayesian analysis to describe a quantity from 0 to 1, such as a probability or proportion quantifying our knowledge for another distribution. ~~And~~ it is very flexible to capture unimodal asymmetric densities in 0 to 1.  -->

However, \hh{a single Beta density might not be sufficient} to explain a heavy tail or  a second mode such as the one in the KM distribution in \autoref{fig:example}. We therefore further consider the beta mixture distribution. <!--which is a more complicated distribution than the beta distribution as a special case.-->
In the beta mixture distribution, we introduce a weighted sum of prior probabilities to combine several beta densities as one:


\begin{definition}[Beta distribution]
Random variable $X$ is said to follow a Beta distribution, $X \sim B(\mu, \phi)$, with real-valued parameters $\mu \in (0,1)$ and $\phi > 0$, if its density can be written as
\begin{align*}
f(x;\mu, \phi) =  \frac{\Gamma(\phi)}{\Gamma(\mu\phi)\Gamma\left((1-\mu)\phi\right)}x^{\mu\phi-1}(1-x)^{(1-\mu)\phi-1},
\end{align*}
where $\Gamma(.)$ is the Gamma function.In this parameterization, $E[X] = \mu$ and Var$[X] = \mu(1-\mu) / (\phi +1)$.
\hh{$\mu$ is the location parameter of the distribution, $\phi$ is called the `sample size' or precision. }
\end{definition}


Note that we are using the mean and precision parameterization of beta distributions which simplifies the math in calculation and is more intuitive. This parameterization is equivalent to the more  usual  parameterization using shape parameters $\alpha$ and $\beta$ through the following transformation:

\begin{align*}
\mu = \frac{\alpha}{\alpha+\beta}\ \ \ \text{ and } \ \ \ 
\phi = \alpha + \beta
\end{align*}

\begin{definition}[Beta-Mix distribution]
Random variable $Y$ is said to follow a $k$-component beta mixture distribution $Y \sim  \text{Betamix}(p, \mu, \phi)$ with parameter vectors $p$, $\mu$ and $\phi$, if its density is written as:

\begin{align*}
f(y;p, \mu, \phi) =& \sum_{\ell=1}^k p_{\ell} \cdot f_\ell(y;\mu_{\ell}, \phi_{\ell}) 
\end{align*}
where
$f_\ell(\cdot;\mu_{\ell}, \phi_{\ell})$, $\ell = 1, 2, 3, ..., k$ are beta densities
with parameter vectors
$p$, $\mu$ and $\phi$. For these parameters holds: $p \in S_{k+1}$, i.e. $p \in R^{k}$,  all elements in $p$ are positive and sum to 1:  $0 \le p_{\ell} \le 1, \forall \ell \in {1, 2, 3, ..., k}$ and $\sum_{\ell = 1}^k p_{\ell} = 1$.

$\mu_{l}$ and $\phi_{l}$ are distribution parameters for $l^{\text{th}}$ component:
\begin{align*}
&\mu = (\mu_{1}, \mu_{2}, \mu_{3}, ... , \mu_{k}),        &\mu_{\ell} \in (0, 1),~~ l = 1, 2, 3, ..., k \\
&\phi = (\phi_{1}, \phi_{2}, \phi_{3}, ... , \phi_{k}),    &\phi_{\ell} \in (0, +\infty),~~ l = 1, 2, 3, ..., k
\end{align*}
\end{definition}

\hh{\autoref{fig:betamixFig} is orphaned. You need to refer to it in the text.}

```{r betamixFig, out.width="70%", fig.align="center", fig.cap="Two component beta-mixture distribution illustration examples. One component with (mu = 0.759, phi = 18.476) is held fixed, and the other component with (phi = 15.812) \\hh{The exact parameters of mu and phi are not of interest} has mu changing from 0.1 to 0.466 with the mode moving from left to right at the same time. The prior probability p1 is fixed at 0.419. With the increase of the varying mu, the two modes gradually get closer to form an ideal shape for the KM distribution. The parameters are inspired by the later section estimation", fig.pos="h"}

p1 <- 0.419
mu1 <- 0.466
phi1 <- 15.812
mu2 <- 0.759
phi2 <- 18.476

simdata <- function(p1, a1, b1, a2, b2) {
  dcomp1 <- dbeta(0:1000/1000, shape1 = a1, shape2 = b1)
  dcomp2 <- dbeta(0:1000/1000, shape1 = a2, shape2 = b2)
  
  output <- data.frame(x = 0:1000/1000, y = p1*dcomp1 + (1-p1)*dcomp2)
  output
}

seq_mu1 <- seq(from = 0.1, to = 0.466, length.out = 7)
params <- data.frame(p1 = p1, mu1 = seq_mu1, phi1 = phi1, mu2 = mu2, phi2 = phi2)
comdata <- lapply(seq_along(seq_mu1), FUN = function(i) {
  p1 <- params$p1[i]
  mu1 <- params$mu1[i]
  phi1 <- params$phi1[i]
  mu2 <- params$mu2[i]
  phi2 <- params$phi2[i]
  output <- simdata(p1, a1 = mu1*phi1, b1 = (1-mu1)*phi1, a2 = mu2*phi2, b2 = (1-mu2)*phi2)
  output
}) %>% bind_rows(.id = "id")

comdata %>%
  ggplot(aes(x = x, y = y, group = id, color = as.numeric(id))) + 
  geom_line() + 
  scale_color_gradient2(low = "black", mid = "grey", high = "darkorange", midpoint = 4) +
  ylab("Density") + 
  xlab("Value") +
  guides(color = guide_none())

```





\yg{Note that in our applications, we assume different beta mixture distributions for KM and KNM cases. And within each case, the scores are assumed independent and identically distributed.}

Error rate analysis
-----------------------------------------

Estimating the underlying theoretical error rates is a fundamental challenge in the forensic science [@PCAST2016; @PCAST2016addendum]. The estimation process becomes more straightforward after we finished the distribution estimation. There are many types of error rates can be defined. We focused on the four most used ones, i.e. 1) False positive rate (FPR), 2) False negative rate (FNR), 3) False identification rate (FIR), 4) False exclusion rate (FER), as defined in Table \ref{Error rate table}. FPR and FNR are considered source-specific assessment. They are both probabilities given the ground truth to make false conclusions, which is useful especially when we evaluate an algorithm. FIR and FER are considered decision-specific assessment. They are both probabilities given the algorithm decisions to false claim the actual source, which is useful in real cases when jury is presented with the forensic conclusions to make judgment.

The error rates are also affected by the threshold selected to make decisions. In an extreme example, if we are very conservative in making identifications, we can set the threshold to be large enough, say CCF >= 0.99, then the FPR will be 0. But at the same time, the FNR will be 1. It's clear that there are trade-offs between the error rates. So, to make these error rates comparable and consistent, for the current paper, we refer the threshold as the value where the probability densities of KM and KNM distributions equal with each other. This is not a bad choice, since it has good statistical meaning that at this value, the likelihood ratio equals 1 and any deviation from 1 would give preference to a certain decision (as if a threshold). For the two component beta mixture distributions we estimated in last section, this threshold is 0.529. In the application of the algorithms, the threshold need to be carefully chosen where the study of associated error rates will in turn be essential guiding that process.

Data
-----------------------------------------

For the following sections of the paper, we will base our analysis on the LAPD data set[reference]. The data set consists of bullets fired from 626 Beretta F/FS firearms with 4 bullets for each barrel. It is the first time such a large data set available to the researchers, which makes it possible for a statistical analysis for the distributions of similarity scores. For the firearms used, there are 6 lands in each barrel which will leave 6 LEAs on the bullets for comparisons. We follow the aforementioned standard procejure to generate CCFs as similarity scores (include the procedure). There are 6 different bullet comparisons within a set of 4 bullets for a barrel. There are 6 same source land comparisons and 30 distinct different source land comparisons in each of those 6 bullet comparisons. In total, for a single barrel, there are 36 same source land to land comparisons. For the different source land to land comparisons, we have much more cases. If we limit the scope within each barrel, there will be $30\times 6 + 15 \times 4 = 240$ land to land comparisons for which we also take the comparisons among lands on a same bullet into consideration.

Database model v.s. case specific model
------------------------------------------

### Study Design

In the current practice, FTEs  fire two or three test bullets from a suspected firearm using ammunition that matches the evidence found on the crime scene as closely as possible.
Test fires are checked for consistency of striation marks between fires. Generally, the bullet with the more well expressed striations is used to compare to the evidence. With an automated algorithm in these cases as a tool helping the examiners, we will make use of all those available test bullets and lands to generate similarity scores. And those scores will be used to get score-based likelihood ratios which will quantify the strength of evidence supporting either hypothesis from the prosecution side or the defendant side. The critical step of producing likelihood ratio is to get estimations of the distributions of both KM and KNM comparisons.

Here, we distinguish between two scenarios: 1) use scores from comparisons of test fires to estimate densities for same source and different source, and 2) use scores with known provenance from a database of similar ammunition and firearms. The second scenario is where our analysis of the theoretical distributions of the relevant population of similarity scores stand out. For a similar combination of firearms and ammunition, the similarity scores extracted through a same algorithm are considered identically distributed and comprise the relevant population, which is not a bad idea. In real cases, as stated, we can only make use of similarity scores from two or three test fires. This is good enough to produce valid comparisons but likely not good enough to estimate the distributions which generate the likelihood ratios. Besides the likelihood ratio calculation, the reference distributions are also important in deciding thresholds and control the error rates. For thresholds specifically, we choose the point where the likelihood ratio is 1 which has no preference for either hypothesis.

In either of the two scenarios, we derive scores by comparing test fires and the questioned bullet. The pipeline of the process is as follows: 1) given a questioned bullet and a suspected firearm, 2) conduct test fires with the suspected firearm and collect three test fired bullets, 3) for every pair of bullets (including the questioned), we conduct all possible land comparisons and get 36 land comparison scores (6 land engraved areas on each bullet), 4) among 36 comparisons within a pair of bullets, we select the six comparisons as KM comparisons (for comparisons among test fired bullets) or questioned land comparisons (for comparisons between the questioned bullet and any test fired bullet) by SAM [reference] due to the nature order of land engraved areas produced by a same barrels. Then, for the first scenario, the KM and KNM distributions will be estimated with the KM and KNM similarity scores among the test fired bullets respectively. While for the second scenario, the distributions are estimated with a database of the similarity scores from similar firearms and ammunition.

### Training and testing set

To show the usefulness of the relevant population and the better distribution estimations by using a database, we design an experiment with our LAPD data set. We randomly split our data into the database sets and case sets. For the database sets, we conduct every comparison within each set (among four bullets), and estimate the KM and KNM reference distribution as two component beta mixture distributions with all available data. For the case sets, we randomly choose one bullet out of each set (four bullets a set) as a questioned bullets and leave the other three as test fired bullets. By compare the questioned bullet with the other three from the sets, we get known-matched bullet comparisons. By randomly matching the questioned bullets with the test fired bullets (except its own set and simple exchanges of bullets between two sets), we get known-non-matched bullet comparisons. One important question is how many sets of bullets should be assigned as database sets since we would like to keep it small but sufficient and put more data into case sets to evaluate the performance. And also, this is an important question in general that how many test fires do we need in general firearm forensic practice. We will dig into this issue in the later section. But for now, we split the 442 sets into 342 database sets and 100 test sets.

Following the above procedure, we have 100 questioned bullets (same bullets in same and different source cases), 100 same source question-test pairs, 100 different source question-test pairs. For each questioned bullet in either case (same or different source), we have 18 questioned land comparisons. For the database scenario, we have 342x36 = 12312 known matched land comparisons, and 342x240 = 82080 known non-matched land comparisons. For the individual case scenario, we have 18 known matched land comparisons (among test bullets), and 125 known non-matched land comparisons. [may be summarized in a table, and for the specific counts, we will provide details of that when introducing the LAPD data set]

Sample size effect and convergence
----------------------------------------

To address the question that how many test fires we need to reach good estimations of the distributions in a database and finally, a good classification result with controlled error rates, we will investigate the estimations and error rates in a changing sample size setting. There are two ways we might evaluate the estimations. First, from a statistical view, we can quantify the variation of the estimated curves, and find the minimum requirement to make the estimation converge. From a practical view, we look at the error rate and find a sample size when the error rates converge. Then, we can make sure to get reliable estimations of the densities and reliable performance of making predictions with likelihood ratios.

We design a study to evaluate the effect of data sizes. As in the previous sections, we are using the LAPD data sets. And set the unit of data size as a set of four same source bullets as in the case of LAPD and in general, the way of forming a data base. It's worth noting that any meaningful statistical results rely on replications. But we must be careful that as the used sample size in a single trial increases, the replications can require very large data sets. Otherwise, the heavy reuse of data could cause high dependence among the results and lead to false conclusions. We are using a approach that is adopted in machine learning field of dealing with this issue. Instead of using different samples at each fixed sample size to achieve replications, we are adding new data to the existing pool of used data to achieve replications by sequentially fitting the model. This also meets the sequential convergence pattern in a statistical sense perfectly. When the addition of new data doesn't significantly change some adopted measurements of our goal, then we find the minimum number of sample size reaching that stage. And additionally, we also repeat the above process with ten randomly selected sequences, which further investigates the data specific effect in the process and it is harder to see all the sequences converged uniformly to a same reasonable small interval. So we also introduce different levels of convergence to formulate the conclusions.

\yg{need to move the selected measurements of convergence up too, which is now mixed with the results...}

Results and discussion
========================================

Model result
------------------------------------

<!-- \yg{For the following sections of the paper, we will base our analysis on the LAPD data set[reference]. The data set consists of bullets fired from 626 Beretta F/FS firearms with 4 bullets for each barrel. It is the first time such a large data set available to the researchers, which makes it possible for a statistical analysis for the distributions of similarity scores. For the firearms used, there are 6 lands in each barrel which will leave 6 LEAs on the bullets for comparisons. We follow the aforementioned standard procejure to generate CCFs as similarity scores (include the procedure). There are 6 different bullet comparisons within a set of 4 bullets for a barrel. There are 6 same source land comparisons and 30 distinct different source land comparisons in each of those 6 bullet comparisons. In total, for a single barrel, there are 36 same source land to land comparisons. For the different source land to land comparisons, we have much more cases. If we limit the scope within each barrel, there will be} $30\times 6 + 15 \times 4 = 240$ \yg{land to land comparisons for which we also take the comparisons among lands on a same bullet into consideration.} -->

<!-- We consider cross correlation functions (CCF) for land comparisons, which is produced by calculating the maximized CCF between two signatures extracted from a pair of bullet LEAs. -->

The estimation was done using Nelder-Mead algorithm in R [reference]. This is a general purpose numeric method which works reasonably well for multidimensional optimization problems. In our case, the objective function is the log likelihood function of the beta mixture distribution. Therefore, we finally found the maximum likelihood estimates (MLE). \yg{We started with estimating all single-component, two-component and three-component beta mixture models for both KM and KNM distributions. By comparing the models, we could see how well they fit the data and test if some components are necessary.} The estimated beta mixture distributions for KM and KNM are in Table \ref{Full Data KM Distribution Estimation} and Table \ref{Full Data KNM Distribution Estimation} respectively. For the single-component beta distributions, the KM distribution has a mean at 0.635 and the KNM distribution has a mean at 0.404. We can also see that for the two-component beta mixture distributions for both KM and KNM, there are components with mean around 0.5 and the other components are more separated from each other. This indicates the model successfully accounted for different situations of comparisons involving tank rash and random identification etc., which is an ideal property we would like to see and make use of in explaining the similarity scores. \yg{Another point about the estimates that is worth mentioning is that the $\phi$ of the KM beta distribution is much smaller than any of the two components of the two-component KM beta mixture distribution. This indicates that the two components well accounted for each mode locally with much smaller variation than the overall single-beta distribution which failed to capture the local structure and the heavy tail.}


\begin{table}[h]
\centering
\begin{tabular}{ |p{1.3cm}||p{2.1cm} p{0.8cm} p{0.8cm} p{0.8cm}p {1.2cm} p{1.2cm}|}
 \hline
 \multicolumn{7}{|c|}{Full Data KM Distribution Estimation} \\
 \hline
 Model & Component& $\mu$ & $\phi$ & logLik & p-value& BIC\\

       & Prior& & & & &\\

       & Probability& & & & & \\
 \hline
 \multirow{3}{*}{3-comp} &  0.247 &0.406&   24.097 & \multirow{3}{*}{6278} & &\multirow{3}{*}{-12479}\\
 & 0.521  & 0.664 & 13.342& & &\\
 & 0.232 &0.819&   33.823 & & &\\
 \hline
 \multirow{2}{*}{2-comp} & 0.419  & 0.466 & 15.812 & \multirow{2}{*}{6243} & \multirow{2}{*}{0}&\multirow{2}{*}{-12438}\\
 & 0.581 &0.759&   18.476 & & &\\
 \hline
 Beta &  & 0.635 & 6.529 & 5712 & 0&-11404\\
 \hline
\end{tabular}
\caption{Parameter estimations for the Beta distribution (1-component beta mixture distribution), 2-component and 3-component distributions for the KM CCF. "2-comp" refers to the 2-component beta mixture distribution, and the same for "3-comp". Column "logLik" is the maximized log likelihood for each distribution. Column "p-value" is the p-value for asymptotic likelihood ratio tests between the current model and one-step more complex model, which suffers from the large sample size and the resulting over-stated actual significance}
\label{Full Data KM Distribution Estimation}

\end{table}

<!-- Parameter estimations for the Beta distribution (1-component beta mixture distribution), 2-component and 3-component distributions for the KM CCF. "2-comp" refers to the 2-component beta mixture distribution, and the same for "3-comp". Column "logLik" is the maximized log likelihood for each distribution. Column "p-value" is the p-value for asymptotic likelihood ratio tests between the current model and one-step more complex model, where 0 indicates that we would reject the hypothesis that the current one is sufficient to decribe the data -->

\begin{table}[h]
\centering
\begin{tabular}{ |p{1.3cm}||p{2.1cm} p{0.8cm} p{0.8cm} p{0.8cm}p {1.2cm} p{1.2cm}|}
 \hline
 \multicolumn{7}{|c|}{Full Data KNM Distribution Estimation} \\
 \hline
 Model & Component& $\mu$ & $\phi$ & logLik & p-value& BIC\\

       & Prior& & & & &\\

       & Probability& & & & & \\
 \hline
 \multirow{3}{*}{3-comp} &  0.650 &0.345&   35.017 & \multirow{3}{*}{78961} & &\multirow{3}{*}{-157830}\\
 & 0.314  & 0.490 & 24.979& & &\\
 & 0.032 &0.667&   18.454 & & &\\
 \hline
 \multirow{2}{*}{2-comp} & 0.674  & 0.358 & 39.908 & \multirow{2}{*}{78808} & \multirow{2}{*}{0}&\multirow{2}{*}{-157558}\\
 & 0.336 &0.494&   13.324& & &\\
 \hline
 Beta &  & 0.404 & 15.801 & 75349 & 0&-150675\\
 \hline
\end{tabular}
\caption{Parameter estimations for the Beta distribution (1-component beta mixture distribution), 2-component and 3-component distributions for the KNM CCF. "2-comp" refers to the 2-component beta mixture distribution, and the same for "3-comp". Column "logLik" is the maximized log likelihood for each distribution. Column "p-value" is the p-value for asymptotic likelihood ratio tests between the current model and one-step more complex model, which suffers from the large sample size and the resulting over-stated actual significance}
\label{Full Data KNM Distribution Estimation}

\end{table}

The estimated beta distributions are shown in \autoref{fig:singlebeta} and the estimated two-component beta mixture distributions are also shown in \autoref{fig:two-component-betamix}. As expected, the estimated distributions show the properties we desire for CCFs. 
\yg{For a given value 0.5 (which is not a special one), the KNM distribuion put a large probabilitiy for values under 0.5, while the KM distribution put a large probablity for values above 0.5, indicating the degree of seperation could be achieved by the observed values from the two distributions. However, both distributions put some probabilties on the other side of 0.5 where most realizations of the CCFs are from a different distribution. We could then use the likelihood ratio based on the distributions we have to evaluate the results in that case.}
<!-- The majorities of the estimated distributions are apart from each other, while the minority part of the two distributions has some overlap.  -->
It's worth noting that both curves have heavy tails to the farther boundaries \yg{from the modes} and the KM curve has a heavier tail compared to KNM. We can see the two-component distributions fit the data very well, while the single beta distributions are not as good as the two-component distributions for both KM and KNM. The single beta distribution for KM clearly failed to capture the potential second mode of the histogram indicating it is not sufficient. The single beta distribution for KNM captured the mode but still failed to capture the distributional information for some parts. This indicates that even though beta distributions are flexible for variables in 0 to 1, they are still too restricted to be able to describe the cases here. The two-component beta mixture distributions are more promising.


```{r singlebeta, out.width="70%", fig.align="center", fig.cap="Estimated beta distributions for full data", fig.keep="hold", fig.pos="h"}
myfit3n4_simulation %>%
  ggplot(aes(color = Class, fill = Class)) + 
  geom_line(aes(x = ccf, y = y)) + 
  geom_histogram(aes(x = ccf, y = ..density..), bins = 20, color = "gray99",
                 position = "identity", alpha = 0.5, 
                 data = bind_rows(ccf_km_full_data, ccf_knm_full_data, .id = "Class")) +
  # scale_color_discrete(breaks = c("1", "2"), label = c("KM", "KNM")) + 
  # scale_fill_discrete(breaks = c("1", "2"), label = c("KM", "KNM")) +
  scale_color_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  scale_fill_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  ylab("density")
```


```{r two-component-betamix, out.width="70%", fig.align="center", fig.cap="Estimated two-component beta mixture distributions for full data", fig.pos="h"}
myfit1n2_simulation %>%
  ggplot(aes(color = Class, fill = Class)) + 
  geom_line(aes(x = ccf, y = y)) + 
  geom_histogram(aes(x = ccf, y = ..density..), bins = 20, color = "gray99",
                 position = "identity", alpha = 0.5, 
                 data = bind_rows(ccf_km_full_data, ccf_knm_full_data, .id = "Class")) +
  # scale_color_discrete(breaks = c("1", "2"), label = c("KM", "KNM")) + 
  # scale_fill_discrete(breaks = c("1", "2"), label = c("KM", "KNM")) +
  scale_color_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  scale_fill_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  ylab("density")
```

Three candidate distributions for each of KM and KNM are considered. For the increasing of complexity, we have single beta, two-component beta mixture and three-component beta mixture distributions. And they are also nested in that order. Naturally, we first look at the maximized log-likelihood of each model, and we can do asymptotic log-likelihood ratio chi-square tests for single beta against two-component beta mixture distributions, and two-component against three-component beta mixture distributions. The p-values of those test are shown in the Table \ref{Full Data KM Distribution Estimation} and Table \ref{Full Data KNM Distribution Estimation} as the column "p-value". Surprisingly (or not), we found all those p-values are 0 which strongly suggests a more complex model when there is one. However, considering the size of the data we are using to fit these models, we can expect the statistical significance will be easily achieved since any small difference of the sufficiency will be detected. So we have to take the sample size effect into consideration. As Bayesian information criterion (BIC) is a well used criterion which takes the model complexity, sufficiency and the sample size (by a log function) into account. As shown in the column "BIC" in the tables, the BICs for the single beta distributions are larger than that of the two-component beta mixtures by relatively large proportions. And the BICs for the two-component beta mixture distributions are a little larger than that of the three-component beta mixture distributions for both KM and KNM. Obviously, we would prefer the two-component beta mixture distributions over the single beta distributions, but we would cast a doubt when it comes to the three-component beta mixture distributions. We still prefer the two-component distributions instead of the three-component distributions. The reasons are: 1) these differences of BICs between two and three component distributions are really small in proportion (by 0.3% for KM and 0.017% for KNM), 2) the estimation cost is not accounted, which will be higher for a more complex model, 3) the BICs still don't take the sample size effect fully into account since the log function for sample size goes to flat when the sample sizes are large. By simple calculation, we can see that the log-likelihood increased by a multiplicative factor more than 10 when the sample sizes increased from KM case to KNM case, however, at the same time, the log of sample size only roughly increased from 9 to 11. So we choose the two-component beta mixture distributions for both KM and KNM. And also as we have seen, these distributions have potential good forensic interpretations. 

Itâ€™s also helpful to see how the individual components look like in the beta mixture distributions as in \autoref{fig:components}. The KNM and KM distributions seem to share a common component while keeping the other components far apart from each other. The separated components represent the ideal cases where the bullet land engraved areas preserved the information of the source well and result in clearly distinct separation. The shared components represent the cases where the KM results in lower scores because of some degree of tank rash, pitting, breakoff or other damages on the bullets and the KNM results in higher score because of the random identification effect. According to the estimated prior probabilities, both distributions put less weight on the common component while putting larger weight on the the components characterizing the differences of KM and KNM respectively, which agrees on our expectation that majorities of the distributions are separated while the minorities overlapped. These properties together well explained the observed empirical distribution of similarity scores in our cases. Particularly, the heavier tail of the KM comparisons is explicitly included in the form of the model by one of the components.

```{r components, out.width="70%", fig.align="center", fig.cap="Estimated-components", fig.keep="hold", fig.pos="h"}
myfit1n2_simulation %>%
  ggplot(aes(color = Class)) + 
  geom_line(aes(x = ccf, y = y1))  +
  geom_line(aes(x = ccf, y = y2))  +
  #scale_color_discrete(breaks = c("1", "2"), label = c("KM", "KNM")) + 
  scale_color_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  ylab("density")
```

Error rate result
----------------------------------------

- Given the iid assumption, could we develop a further model for the bullet level distribution and estimate the corresponding error rates?

<!-- Estimating the underlying theoretical error rates is a fundamental challenge in the forensic science [@PCAST2016; @PCAST2016addendum]. The estimation process becomes more straightforward after we finished the distribution estimation. There are many types of error rates can be defined. We focused on the four most used ones, i.e. 1) False positive rate (FPR), 2) False negative rate (FNR), 3) False identification rate (FIR), 4) False exclusion rate (FER), as defined in Table \ref{Error rate table}. FPR and FNR are considered source-specific assessment. They are both probabilities given the ground truth to make false conclusions, which is useful especially when we evaluate an algorithm. FIR and FER are considered decision-specific assessment. They are both probabilities given the algorithm decisions to false claim the actual source, which is useful in real cases when jury is presented with the forensic conclusions to make judgment. -->

\begin{table}[h]
\centering
\begin{tabular}{ |p{1.8cm}||p{3.2cm} |p{3.1cm}| p{2.5cm}|}
 \hline
 &\multicolumn{2}{c|}{Ground Truth}&  \\
 \hline
    & Match & Non-Match&Error Rate\\
 \hline
 Identification & Correctly judged as matches, True Positives (TP) & Incorrectly judged as matches, False Positive (FP)&False ~~~~~~~~~~~~~Identification Rate (FP/(TP + FP))\\
 \hline
 Elimination & Incorrectly judged as non-matches, False Negative (FN)& Correctly judged as non-matches, True Negative (TN)&False Exclusion Rate (FN/(TN + FN))\\
 \hline
 Error Rate & False Negative Rate (FN/(FN+TP)) & False Positive Rate (FP/(FP + TN))&\\
 \hline
\end{tabular}
\caption{xxxx}
\label{Error rate table}

\end{table}

<!-- The error rates are also affected by the threshold selected to make decisions. In an extreme example, if we are very conservative in making identifications, we can set the threshold to be large enough, say CCF >= 0.99, then the FPR will be 0. But at the same time, the FNR will be 1. It's clear that there are trade-offs between the error rates. So, to make these error rates comparable and consistent, for the current paper, we refer the threshold as the value where the probability densities of KM and KNM distributions equal with each other. This is not a bad choice, since it has good statistical meaning that at this value, the likelihood ratio equals 1 and any deviation from 1 would give preference to a certain decision (as if a threshold). For the two component beta mixture distributions we estimated in last section, this threshold is 0.529. In the application of the algorithms, the threshold need to be carefully chosen where the study of associated error rates will in turn be essential guiding that process. -->

The four theoretical error rates are directly associated with the distributions we estimated as shown in the \autoref{fig:fnr-fpr} and \autoref{fig:fir-fer}. The estimated values of the error rates are reported in Table \ref{estimated-error-rates}. The FPR is 0.148, while the FNR is 0.301 which is much larger. This means for the given threshold value 0.529, the algorithm has probability 0.148 to misclssify an actual different-sourced pair of land engraved areas (LEAs) into identifications. And the algorithm has a probability 0.301 to misclassify an actual same-sourced pair of land engraved areas (LEAs) into eliminations. The fact that FNR is larger than FPR implies that for the current threshold, we are more likely to misclassify an pair of KM LEAs instead of KNM LEAs. This is partly due to the fact the KM distribution is more spread than the KNM distribution. The FPR will decrease rapidly when increasing the threshold, while the FNR will be small only when the threshold is very low at great cost of FPR. And similar pattern can be observed from FIR and FER. However, these error rates are explained in a totally different view. FIR is 0.212 means when the algorithm reports an identification (to the jury in court potentially), the algorithm could have made wrong claim at probability 0.212. As we aforementioned, the FPR and FNR are used to evaluate algorithms overall while the FIR and FER are used to evaluate a certain result reported.

<!-- 1) False positive rate -->
<!-- 2) False negative rate -->
<!-- 3) Negative predictive value/ False identification rate -->
<!-- 4) False omission rate/ False exclusion rate -->


```{r fnr-fpr, out.width="70%", fig.align="center", fig.cap="False Negative Rate(FNR) and False Positive Rate(FPR)", fig.keep="hold", fig.pos="h"}
# prepare the polygon
polydata_fnr <- myfit1n2_simulation %>% 
  filter(ccf<=0.53, Class == "1") %>%
  select(ccf, y) %>%
  bind_rows(data.frame(ccf = 0.53, y = 0))
polydata_fpr <- myfit1n2_simulation %>% 
  filter(ccf>=0.53, Class == "2") %>%
  select(ccf, y) %>%
  bind_rows(data.frame(ccf = 0.53, y = 0))
polydata_fnrfpr <- bind_rows(polydata_fnr, polydata_fpr, .id = "Class")

myfit1n2_simulation %>%
  ggplot() + 
  geom_polygon(aes(x = ccf, y = y, fill = Class), data = polydata_fnrfpr, alpha = 0.5) +
  geom_vline(xintercept = 0.529, linetype = 2) +
  geom_line(aes(x = ccf, y = y, color = Class)) +  
  geom_text(aes(x = ccf, y = y, label = label), data = data.frame(ccf = 0.72, y = 3, label = "Threshold = 0.529")) + 
  geom_text(aes(x = ccf, y = y, label = label), data = data.frame(ccf = c(0.375, 0.6), y = c(0.5, 0.45), label = c("FNR", "FPR"))) + 
  # scale_color_discrete(breaks = c("1", "2"), label = c("KM", "KNM")) + 
  # scale_fill_discrete(breaks = c("1", "2"), label = c("KM", "KNM")) +
  scale_color_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  scale_fill_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  ylab("density")
```


```{r fir-fer, out.width="70%", fig.align="center", fig.cap="False Indentification Rate(FIR) and False Exclusion Rate(FER)", fig.keep="hold", fig.pos="h"}
# prepare the polygon
polydata_part3 <- myfit1n2_simulation %>%
  filter(ccf<=0.53)

polydata_part3[1:53,] <- polydata_part3[1:53,] %>% arrange(desc(ccf))

polydata_part4 <- myfit1n2_simulation %>%
  filter(ccf>=0.53) 

polydata_part4[1:47,] <- polydata_part4[1:47,] %>% arrange(desc(ccf))

polydata_part34 <- bind_rows(polydata_part3 %>% select(ccf, y), polydata_part4 %>% select(ccf, y), .id = "Class")
polydata_part34 <- polydata_part34 %>% mutate(Class = as.character(as.numeric(Class) + 2))

polydata <- bind_rows(polydata_fnrfpr, polydata_part34)

polydata %>% 
  ggplot() + 
  geom_polygon(aes(x = ccf, y = y, group = Class), alpha = 0.1) +
  geom_text(aes(x = ccf, y = y, label = label), 
            data = data.frame(ccf = c(0.375, 0.6, 0.375, 0.75), y = c(0.5, 0.45, 2, 1), label = c("A1", "A2", "A3", "A4"))) + 
  geom_line(aes(x = ccf, y = y, color = Class), data = myfit1n2_simulation) +
  geom_vline(xintercept = 0.529, linetype = 2) +
  geom_text(aes(x = ccf, y = y, label = label), data = data.frame(ccf = 0.72, y = 3, label = "Threshold = 0.529")) + 
  geom_text(aes(x = ccf, y = y, label = label), 
            data = data.frame(ccf = c(0.15, 0.15), y = c(3.5, 3.2), label = c("FIR = A2/(A2+A4)", "FER = A1/(A1+A3)")))+
  # scale_color_discrete(breaks = c("1", "2"), label = c("KM", "KNM")) + 
  scale_color_manual(breaks = c("1", "2"), label = c("KM", "KNM"), values = c("darkorange", "darkgrey")) +
  ylab("density")
```

\begin{table}[h]
\centering
\begin{tabular}{ |p{4.5cm}|p{3cm}|}
 \hline
 Type of Error Rates & Estimated Values\\
 \hline
 False positive rate (FPR) & ~~~~~~0.148\\
 \hline
 False negative rate (FNR)& ~~~~~~0.301\\
 \hline
 False identification rate (FIR)& ~~~~~~0.212\\
 \hline
 False exclusion rate (FER)& ~~~~~~0.354\\
 \hline
\end{tabular}
\caption{xxxx}
\label{estimated-error-rates}

\end{table}

Overall, the error rates are not small enough to give confidence to any conclusions. But remember the current comparison discussed is in the land level. In real cases, we will need to combine the land level conclusions to bullet level ones. And this process of combining multiple pieces of evidence will improve the performance greatly. One of the common ways in computer-based bullet analysis is to calculate the sequential average maximum (SAM) [susan's paper and the original paper of SAM?] of land scores. Next, we are going to making use of the KM and KNM distributions in the land level to explore the error rates of the bullet level.

Database model v.s. case specific model result
--------------------------------------------

<!-- In the current practice, FTEs  fire two or three test bullets from a suspected firearm using ammunition that matches the evidence found on the crime scene as closely as possible. -->
<!-- Test fires are checked for consistency of striation marks between fires. Generally, the bullet with the more well expressed striations is used to compare to the evidence. With an automated algorithm in these cases as a tool helping the examiners, we will make use of all those available test bullets and lands to generate similarity scores. And those scores will be used to get score-based likelihood ratios which will quantify the strength of evidence supporting either hypothesis from the prosecution side or the defendant side. The critical step of producing likelihood ratio is to get estimations of the distributions of both KM and KNM comparisons. -->

<!-- Here, we distinguish between two scenarios: 1) use scores from comparisons of test fires to estimate densities for same source and different source, and 2) use scores with known provenance from a database of similar ammunition and firearms. The second scenario is where our analysis of the theoretical distributions of the relevant population of similarity scores stand out. For a similar combination of firearms and ammunition, the similarity scores extracted through a same algorithm are considered identically distributed and comprise the relevant population, which is not a bad idea. In real cases, as stated, we can only make use of similarity scores from two or three test fires. This is good enough to produce valid comparisons but likely not good enough to estimate the distributions which generate the likelihood ratios. Besides the likelihood ratio calculation, the reference distributions are also important in deciding thresholds and control the error rates. For thresholds specifically, we choose the point where the likelihood ratio is 1 which has no preference for either hypothesis. -->

<!-- In either of the two scenarios, we derive scores by comparing test fires and the questioned bullet. The pipeline of the process is as follows: 1) given a questioned bullet and a suspected firearm, 2) conduct test fires with the suspected firearm and collect three test fired bullets, 3) for every pair of bullets (including the questioned), we conduct all possible land comparisons and get 36 land comparison scores (6 land engraved areas on each bullet), 4) among 36 comparisons within a pair of bullets, we select the six comparisons as KM comparisons (for comparisons among test fired bullets) or questioned land comparisons (for comparisons between the questioned bullet and any test fired bullet) by SAM [reference] due to the nature order of land engraved areas produced by a same barrels. Then, for the first scenario, the KM and KNM distributions will be estimated with the KM and KNM similarity scores among the test fired bullets respectively. While for the second scenario, the distributions are estimated with a database of the similarity scores from similar firearms and ammunition. -->

<!-- To show the usefulness of the relevant population and the better distribution estimations by using a database, we design an experiment with our LAPD data set. We randomly split our data into the database sets and case sets. For the database sets, we conduct every comparison within each set (among four bullets), and estimate the KM and KNM reference distribution as two component beta mixture distributions with all available data. For the case sets, we randomly choose one bullet out of each set (four bullets a set) as a questioned bullets and leave the other three as test fired bullets. By compare the questioned bullet with the other three from the sets, we get known-matched bullet comparisons. By randomly matching the questioned bullets with the test fired bullets (except its own set and simple exchanges of bullets between two sets), we get known-non-matched bullet comparisons. One important question is how many sets of bullets should be assigned as database sets since we would like to keep it small but sufficient and put more data into case sets to evaluate the performance. And also, this is an important question in general that how many test fires do we need in general firearm forensic practice. We will dig into this issue in the later section. But for now, we split the 442 sets into 342 database sets and 100 test sets. -->

<!-- Following the above procedure, we have 100 questioned bullets (same bullets in same and different source cases), 100 same source question-test pairs, 100 different source question-test pairs. For each questioned bullet in either case (same or different source), we have 18 questioned land comparisons. For the database scenario, we have 342x36 = 12312 known matched land comparisons, and 342x240 = 82080 known non-matched land comparisons. For the individual case scenario, we have 18 known matched land comparisons (among test bullets), and 125 known non-matched land comparisons. [may be summarized in a table, and for the specific counts, we will provide details of that when introducing the LAPD data set] -->

The results of the experiment show us how the database scenario with well established density estimations improve the over-all accuracy. As shown in \autoref{fig:matched-km}, the left boxplot represents the the individual case scenario where everything is calculated and predicted based on one questioned bullet and three test fired bullets. The right boxplot represents the database scenario where the similarity scores among one questioned bullet and three test fired bullets are evaluated based on a relevant database. \autoref{fig:matched-km} is based on the matched bullets, in which case all the comparisons of lands are actually matches. For each boxplot, there are one hundred points (number of questioned bullets), and each point represents the proportion of correct predictions (i.e. matched lands) of one questioned bullet and the corresponding three test fired bullets based on total eighteen land-land comparisons as shown in y-axis. \autoref{fig:non-matched-result} is formed similar to \autoref{fig:matched-km}, but all the land-land comparisons are non-matched comparisons based on non-matched bullets. And in \autoref{fig:non-matched-result} the y-axis is the proportion of correctly predicted non-matched lands. The results of the experiments are very strong support of making use of a relevant database for automated algorithms. In \autoref{fig:matched-km}, the evaluation based on an individual set are a little better than using of the database because of its more aggressive and data specific way of calculating likelihood ratio and selection of thresholds. But that improvement is little comparing with the significant loss of the accuracy in correctly identifying the non-matched land-land comparisons. \autoref{fig:non-matched-result} implies that it makes no sense of using the individual case calculation to do prediction for non-matched bullet comparisons, while the database scenario works much better and even better than the two results of the matched bullet case, which meets our expectation of a more concentrated KNM distribution than the KM distribution.

```{r matched-km, out.width="70%", fig.align="center", fig.cap="matched-km", fig.keep="hold", fig.pos="h"}
knitr::include_graphics("../code/organized code/generated figures/matched_km.png")
```

```{r non-matched-result, out.width="70%", fig.align="center", fig.cap="non-matched", fig.keep="hold", fig.pos="h"}
knitr::include_graphics("../code/organized code/generated figures/nonmatched_result.png")
```

[boxplot for KM and KNM, two scenarios, error rates]

[overall confusion table]

[compare this section with nex section, for KM, they are pretty close]

[what about thresholds and likelihood ratios themselves]



In this paper, we use the CCF as the similarity score..

We evaluate the accuracy under each scenario by using a comparable set of tests with questioned bullets of known origin. 

\hh{As test set we are using bullets from the LAPD bullet set: 626 barrels with four fired bullets each. }

\yg{Should we consider a binomial decison rule to get a bullet comparison results? if that works well here...}




Sample size effect and convergence result
------------------------------------------------

<!-- To address the question that how many test fires we need to reach good estimations of the distributions in a database and finally, a good classification result with controlled error rates, we will investigate the estimations and error rates in a changing sample size setting. There are two ways we might evaluate the estimations. First, from a statistical view, we can quantify the variation of the estimated curves, and find the minimum requirement to make the estimation converge. From a practical view, we look at the error rate and find a sample size when the error rates converge. Then, we can make sure to get reliable estimations of the densities and reliable performance of making predictions with likelihood ratios. -->

<!-- We design a study to evaluate the effect of data sizes. As in the previous sections, we are using the LAPD data sets. And set the unit of data size as a set of four same source bullets as in the case of LAPD and in general, the way of forming a data base. It's worth noting that any meaningful statistical results rely on replications. But we must be careful that as the used sample size in a single trial increases, the replications can require very large data sets. Otherwise, the heavy reuse of data could cause high dependence among the results and lead to false conclusions. We are using a approach that is adopted in machine learning field of dealing with this issue. Instead of using different samples at each fixed sample size to achieve replications, we are adding new data to the existing pool of used data to achieve replications by sequentially fitting the model. This also meets the sequential convergence pattern in a statistical sense perfectly. When the addition of new data doesn't significantly change some adopted measurements of our goal, then we find the minimum number of sample size reaching that stage. And additionally, we also repeat the above process with ten randomly selected sequences, which further investigates the data specific effect in the process and it is harder to see all the sequences converged uniformly to a same reasonable small interval. So we also introduce different levels of convergence to formulate the conclusions. -->

\autoref{fig:bd} shows the Bhattacharyya distance between each estimated distribution and the full data distribution in both KM and KNM cases for the ten sequence of randomly selected data. Bhattacharyya distance is associated with the the amount of overlap of two distributions. The lower it is, the more overlapped area there is for the two distributions. From \autoref{fig:bd}, the Bhattacharyya distance decreases rapidly with the increase of the size of the used data at the beginning, and it also reveals that those those distributional estimations with one or two sets of data just don't make sense in producing a stable results. But as the sample size increases to about 5 sets of data, the estimations are pretty stable and close the theoretical distributions, and when it increase to about 40 sets of data, the the estimations essentially converged. It's worth noting that the KNM distribution estimations do have outliers in the early stage, which could be the result of the using of the two-component distributions since the two-component distributions are better with large data as in the previous analysis but with less data, this improvement could be less the cost of estimating a more complex model.(So we will also need to check the single beta) So for the KNM distribution, we might need more data to achieve stable estimations of the two-components distributions, but we do have smaller distance for KNM than KM for estimations with more than 40 sets of data even though both of them are small.

\autoref{fig:threshold} shows the thresholds we calculated from using the pair of KM and KM distributions with increasing sample size and the red line is the threshold we calculated with the full data distributions (considerred as theoretical ones). Similar to the case of Bhattacharyya distance, the thresholds are restricted to a small interval after the extreme variation in the first couple of estimations. This is good since we can quickly reach the point to get reasonable thresholds. But it also reveals that the further convergence of the thresholds are difficult. Unlike the Bhattacharyya distance, the thresholds will converge to a smaller interval approximately from 0.52 to 0.55 but not a point. However, if we focus on each sequence (line), we can find that after 60 sets of data, they all almost converged to a point. This reveals the different level of convergence, i.e. the convergence of estimations for a particular data base and the convergence of estimations of different databases. The latter is obviously more demanding. But finally, these are all in a reasonable small interval. Another point worth pointing out is that the thresholds seem to require more data than the Bhattacharyya distance. It is expected that the statistical convergence (Bhattacharyya distance) is more strict than the practical convergence (the thresholds), but it is not the case. Since the thresohlds are calculated base on both distributions, the convergence of the thresolds depends on the convergence of both KM and KNM distributions. And it enlarges the variation.

\autoref{fig:error} shows the false negative error rates and the false positive error rates. Similar to the thresholds behavior, the error rate has two stages of converges at about 5 sets and 60 sets of data. But in this figure, the y-axis has the exact practical meaning of error rates and we can quantify how large that variation related to the sample sizes means to us. And after 60 sets of data, the error rates are in reasonably small intervals and the cost of further improvement is high. And we also stress the point of different levels of convergence, a database with 60 sets of data can give predictions almost converged to a number and tens of sets of additional data won't change that significantly. The further convergence of different databases to a same number is more demanding and pretty slow. The range of the interval after 60 sets of data is about 0.045 which means that the most extreme difference of the error rates achieved with different database is 0.04 (larger in false positive and smaller in false negative error rates) for each land to land comparison.


```{r bd, out.width="70%", fig.align="center", fig.cap="Bhattacharyya Distance", fig.keep="hold", fig.pos="h"}
knitr::include_graphics("../code/organized code/generated figures/bd.png")
```


```{r threshold, out.width="70%", fig.align="center", fig.cap="Threshold", fig.keep="hold", fig.pos="h"}
knitr::include_graphics("../code/organized code/generated figures/threshold.png")
```


```{r error, out.width="70%", fig.align="center", fig.cap="Error", fig.keep="hold", fig.pos="h"}
knitr::include_graphics("../code/organized code/generated figures/error.png")
```

To conclude the section, we might ask for 5 sets of bullets as the minimum requirement of a meaningful database. Further, 60 sets of bullets could significantly improve the performance. But further improvement can be very expensive.








(try different model for KNM? like one component?)

(In the study of the both types of convergence, we also study the behaviors of the two components and evaluate the functional form of the distributions as well.)

(The convergence of thresholds depends on the convergence of both curves, more demanding)

(We will reproduce the some results in the previous sections: 1. application section?)

\clearpage

Conclusion
==================================

<!-- (Different measurements of similarities of forensic evidence are proposed, but the strength of those evidence is not clear when we study all those scores in hands. Likelihood ratios are considered as a way to unify the different scores and directly link to the strength of evidence. The distributional form is the key of the likelihood ratio analysis of forensic evidence in determining the densities and the thresholds.) -->

We did a distributional analysis of the similarity scores to guide the use of likelihood ratios in quantifying the strength of forensic evidence. In particular, we picked the CCF as a typical similarity score and established two-component beta mixture distributions as the theoretical forms for both known-matched and known-nonmatched land-land comparisons. Then we showed how effectively the use of those theoretical forms from a relevant population could help in improving the performance of the prediction in forensic practice with likelihood ratios. The false negative error rates based on individual cases are a little bit lower than the ones based on the relevant database, but it comes with extreme false positive error rates. As expected, the predictions based on individual cases face the problem similar to overfitting issues. This result strongly supports using a relevant database for the relevant population in likelihood ratio analyses for forensic science. We analyzed the associated underlying theoretical error rates in using likelihood ratios with the distributional forms we established. Ideally, we would expect to achieve those error rates if the relevant population is accessible. Thus, we could theoretically control the error rates in the practice. Further, we analyzed the amount of data necessary to establish a reasonable database for the relevant population, and finally achieve the target error rates. We introduced different level of convergence for the required data sizes. Intuitively, the requirement for a particular database to produce stable results is lower than the requirement of all databases to produce a same stable result. And we could conclude that for a particular database, the ideal size would be 60 sets of bullets.


\clearpage

References {#references .unnumbered}
==========
